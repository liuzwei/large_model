# 大模型学习笔记

## 1.KNN算法

KNN（ **K-最近邻** ，K-Nearest Neighbors）是一种简单且经典的 **监督学习算法** ，主要用于**分类**和**回归**任务。它的核心思想是： **通过测量样本之间的距离，找到与待预测样本最接近的K个邻居，然后根据这些邻居的标签（分类）或数值（回归）进行投票或平均，得到最终预测结果** 。

### KNN算法流程

1. 计算已知类别数据集中的点与当前点之间的距离
2. 按距离递增次序排序
3. 选取与当前点距离最小的 k 个点
4. 统计前 k 个点所在的类别出现的频率
5. 返回前 k 个点出现频率最高的类别作为当前点的预测分类

### **KNN的关键特性**

1. **懒惰学习（Lazy Learning）**

   KNN不显式训练模型，直接存储所有训练数据。预测时实时计算距离，因此**训练时间复杂度低，但预测时间复杂度高**（尤其数据量大时）。
2. **无显式模型**

   模型本质是“所有训练数据的集合”，预测依赖数据分布。
3. **依赖距离度量**

   距离计算方式（如欧氏距离、余弦相似度）直接影响结果，需根据数据特点选择。
4. **对噪声敏感**

   K值过小（如K=1）时，模型易受噪声点干扰；K值过大会忽略局部特征。

---

### **KNN的优缺点**

| **优点**                 | **缺点**                     |
| ------------------------------ | ---------------------------------- |
| 无需显式训练，简单直观         | 计算成本高（预测时需遍历所有样本） |
| 无需假设数据分布（非参数方法） | 对高维数据敏感（“维度灾难”）     |
| 适合多分类任务                 | 对缺失值敏感                       |
| 可解释性强（可视化决策边界）   | 需手动选择K值和距离度量            |

---

### **KNN的应用场景**

1. **分类任务** ：手写数字识别、垃圾邮件分类、医疗诊断等。
2. **回归任务** ：房价预测、股票价格预测等。
3. **推荐系统** ：基于用户相似度的协同过滤。

---

### 过拟合&欠拟合

#### 过拟合（Overfitting）

*  **原因** ：当选择的**K值过小**（如K=1）时，模型过于依赖训练数据中的局部细节，甚至捕捉到噪声点。此时模型复杂度高，对训练数据拟合“过度”。
*  **表现** ：
  * 训练集上的准确率极高，但测试集或新数据上的表现显著下降。
  * 模型对噪声敏感，泛化能力差。
*  **示例** ：若训练数据中存在少量噪声点，K=1时模型会直接模仿这些噪声，导致预测不稳定。

---

#### 欠拟合（Underfitting

*  **原因** ：当选择的**K值过大**（如接近训练样本总数）时，模型过度平滑，忽略数据的局部特征，仅依赖全局趋势。此时模型复杂度低，无法捕捉数据模式。
*  **表现** ：
  * 训练集和测试集上的准确率均较低。
  * 预测结果过于“平均”，无法反映数据的真实分布。
*  **示例** ：在非线性可分的数据中，若K值过大，模型可能强行将不同类别的数据平均化，导致预测错误。

---



## 2.距离的度量


以下是欧式距离、曼哈顿距离、切比雪夫距离、闵可夫斯基距离、闵氏距离（闵可夫斯基的简称）和马氏距离的定义、区别及应用场景的详细说明：

---

### **1. 欧式距离（Euclidean Distance）**

* **定义** ：两点之间的直线距离，公式为：

  $$
  d = \sqrt {\sum_{i=1}^n (x_i - y_i)^2}
  $$
* **特点** ：
* 最常用的距离度量，直观且符合人类对空间的认知。
* 对数据的量纲敏感（需标准化）。
* **应用场景** ：
* 图像识别（像素间相似度）。
* 自然语言处理（词向量距离）。
* 通用机器学习算法（如KNN、聚类）。

---

### **2. 曼哈顿距离（Manhattan Distance）**

* **定义** ：两点在各坐标轴上的绝对差之和，公式为：
  $$
  d = \sum_{i=1}^n\lvert x_i - y_i \rvert
  $$
* **特点** ：
* 类似城市街区行走的路径（只能沿坐标轴方向移动）。
* 对异常值不敏感（绝对值差比平方差更鲁棒）。
* **应用场景** ：
* 高维稀疏数据（如文本分类中的词袋模型）。
* 路径规划（机器人导航中的网格移动）。

---

### **3. 切比雪夫距离（Chebyshev Distance）**

* **定义** ：两点在各坐标轴上的最大绝对差，公式为：

  $$
  d = max_i \lvert x_i - y_i \rvert
  $$
* **特点** ：
* 关注最大差异维度，忽略其他维度。
* 适用于离散空间（如棋盘上的国王移动）。
* **应用场景** ：
* 图像处理（像素差异的最大值）。
* 异常检测（关注极端值）。

---

### **4. 闵可夫斯基距离（Minkowski Distance）**

* **定义** ：欧式距离和曼哈顿距离的广义形式，公式为：
  $$
  d=(\sum_{i=1}^n\lvert x_i - y_i \rvert^p)^{1/p}
  $$
* $p=1 \rightarrow $ 曼哈顿距离
* $p=2 \rightarrow $ 欧式距离
* $p \rightarrow \infty \rightarrow$ 切比雪夫距离
* **特点** ：
* 通过调整 pp**p** 控制模型对异常值的敏感度。
* 计算复杂度随 pp**p** 增大而增加。
* **应用场景** ：
* 需要灵活调整距离权重的场景（如自定义聚类算法）。

---

### **5. 闵氏距离（Minkowski Distance）**

*  **说明** ：闵氏距离是闵可夫斯基距离的简称，两者完全一致，只是翻译不同。

---

### **6. 马氏距离（Mahalanobis Distance）**

* **定义** ：考虑数据分布的距离度量，公式为：

  $$
  d = \sqrt {(x- \mu)^T S^{-1} (x-\mu)}
  $$
* $\mu$ 是均值向量，$S$ 是协方差矩阵。
* **特点** ：
* 消除特征间的相关性影响。
* 对量纲不敏感（自动标准化）。
* 计算复杂度高（需计算协方差逆矩阵）。
* **应用场景** ：
* 多元正态分布中的异常值检测。
* 特征高度相关的场景（如生物信息学）。

---

### **对比总结**

| **距离类型**       | **公式特点**            | **敏感度**        | **适用场景**         |
| ------------------------ | ----------------------------- | ----------------------- | -------------------------- |
| **欧式距离**       | 平方和开根号                  | 对量纲敏感              | 通用场景（图像、文本）     |
| **曼哈顿距离**     | 绝对值和                      | 对异常值较鲁棒          | 高维稀疏数据、路径规划     |
| **切比雪夫距离**   | 最大值差                      | 关注极端差异            | 图像处理、棋盘移动         |
| **闵可夫斯基距离** | 广义形式（pp**p**可调） | 依赖pp**p**的选择 | 需灵活调整权重的自定义算法 |
| **马氏距离**       | 基于协方差矩阵                | 消除量纲和相关性        | 多元正态分布、特征相关场景 |

---

### **选择建议**

1.  **默认选择** ：欧式距离（简单高效）。
2.  **高维稀疏数据** ：曼哈顿距离（鲁棒性更好）。
3.  **图像/棋盘问题** ：切比雪夫距离（关注局部最大差异）。
4.  **特征相关性** ：马氏距离（需计算协方差矩阵）。
5.  **自定义权重** ：闵可夫斯基距离（调整 pp**p** 值）。

---

### **注意事项**

* **量纲问题** ：使用欧式/曼哈顿距离前需标准化（Z-score或Min-Max）。
* **维度灾难** ：高维数据中所有距离趋近相同，需降维（PCA）或特征选择。
* **计算效率** ：KNN等算法需结合KD树或Ball树优化高维距离计算。


## 3.线性回归

线性回归（Linear Regression）是一种用于建模两个或多个变量之间**线性关系**的统计方法。它的核心目标是找到一条“最佳拟合直线”（在更高维度中是超平面），使得这条线能够尽可能准确地预测或解释因变量（目标变量）与自变量（特征）之间的关系。

单变量线性回归算法的成本函数： 

$$
J(\theta) = J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^m(h(x)^{(i)} - y^{(i)})^2
$$

其中 $(h(x)^{(i)} - y^{(i)})$ 是预测值和实际值的差，故成本就是预测值和实际值的差的平方的平均值，之所以乘以 1/2 是为了计算方便，这个是均方差。有了成本函数，就可以精确地测量模型对训练样本拟合的好坏程度。
